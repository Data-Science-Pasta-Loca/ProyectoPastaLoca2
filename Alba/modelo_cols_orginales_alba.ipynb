{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import itertools # Importando itertools para generar combinaciones de columnas\n",
    "# Importando la función seasonal_decompose para la descomposición de series temporales\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import matplotlib.patches as mpatches\n",
    "import payments_manager as pm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#pm.help()\n",
    "#pm.reset()\n",
    "#pm.init() #debug=True)\n",
    "cr_cp = pm.df('cr_cp')\n",
    "fe_cp = pm.df('fe_cp')\n",
    "#cr_cp.info()\n",
    "#fe_cp.info()\n",
    "\n",
    "df_jo = pm.df('df_jo')\n",
    "#df_jo.info()\n",
    "df_jo = pm.sort(\"df_jo\", [\"id_cr\"]).reset_index()\n",
    "df_jo = df_jo.drop(columns=['index'])\n",
    "\n",
    "#df_jo = df_jo.drop(columns=['Mes_created_at'])\n",
    "df_jo_cp = df_jo.copy()\n",
    "\n",
    "df_jo_cp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_jo_cp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_transformado \u001b[38;5;241m=\u001b[39m \u001b[43mdf_jo_cp\u001b[49m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Convertir columnas datetime a timestamps\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m df_transformado\u001b[38;5;241m.\u001b[39mselect_dtypes(include\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatetime64\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mcolumns:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_jo_cp' is not defined"
     ]
    }
   ],
   "source": [
    "df_transformado = df_jo_cp.copy()\n",
    "\n",
    "# Convertir columnas datetime a timestamps\n",
    "for col in df_transformado.select_dtypes(include=['datetime64']).columns:\n",
    "    df_transformado[col] = df_transformado[col].apply(lambda x: x.timestamp() if pd.notnull(x) else None)\n",
    "    \n",
    "# Convertir columnas slot en enteros\n",
    "df_transformado['created_at_slot'] = pd.to_numeric(df_transformado['created_at_slot'], errors='coerce').astype('Int64')\n",
    "df_transformado['created_at_dow'] = pd.to_numeric(df_transformado['created_at_dow'], errors='coerce').astype('Int64')\n",
    "\n",
    "# Reasignar valores en la columna 'transfer_type' y 'charge_moment' para convertirlo a numéricas\n",
    "df_transformado = df_transformado.copy()\n",
    "df_transformado['transfer_type'] = df_transformado['transfer_type'].map({'regular': 0, 'instant': 1})\n",
    "df_transformado['charge_moment_numeric'] = df_transformado['charge_moment'].fillna('null').map({\n",
    "    'before': 1,\n",
    "    'after': 2,\n",
    "    'null': 0  # Asigna 0 para los valores nulos llenados como 'null'\n",
    "})\n",
    "df_transformado['category'] = df_transformado['category'].fillna('null').map({'rejected': 1, 'month_delay_on_payment': 2, 'null':0})\n",
    "\n",
    "# Descartar columna categorica\n",
    "df_transformado=df_transformado.drop(columns=['charge_moment'])\n",
    "\n",
    "# Convertir timedelta a float en días\n",
    "df_transformado['to_receive_ini'] = df_transformado['to_receive_ini'] / pd.Timedelta(days=1)\n",
    "df_transformado['to_receive_bank'] = df_transformado['to_receive_bank'] / pd.Timedelta(days=1)\n",
    "df_transformado['to_reimbur'] = df_transformado['to_reimbur'] / pd.Timedelta(days=1)\n",
    "df_transformado['to_reimbur_cash'] = df_transformado['to_reimbur_cash'] / pd.Timedelta(days=1)\n",
    "df_transformado['to_end'] = df_transformado['to_end'] / pd.Timedelta(days=1)\n",
    "df_transformado['to_send'] = df_transformado['to_send'] / pd.Timedelta(days=1)\n",
    "\n",
    "# Variable categorica type a numérica\n",
    "df_transformado['type'] = df_transformado['type'].fillna('null').map({\n",
    "    'instant_payment': 1,\n",
    "    'split_payment': 2,    \n",
    "    'incident': 3 ,\n",
    "    'postpone' : 4,\n",
    "    'null' : 0\n",
    "})\n",
    "\n",
    "# Variable categorica recovery_Status a numérica\n",
    "df_transformado['recovery_status'] = df_transformado['recovery_status'].fillna('null').map({\n",
    "    'completed': 1,\n",
    "    'pending': 2,    \n",
    "    'pending_direct_debit': 3 ,\n",
    "    'null' : 0\n",
    "})\n",
    "\n",
    "# Variable categorica stat_cr a numérica\n",
    "df_transformado['stat_cr'] = df_transformado['stat_cr'].fillna('null').map({\n",
    "    'approved': 1,\n",
    "    'money_sent': 2,    \n",
    "    'rejected': 3 ,\n",
    "    'pending': 4,\n",
    "    'transaction_declined': 5,\n",
    "    'waiting_user_information': 6,\n",
    "    'direct_debit_rejected': 7,\n",
    "    'canceled': 8,\n",
    "    'direct_debit_sent': 9,\n",
    "    'waiting_reimbursement': 10,\n",
    "    'active': 11,\n",
    "    'money_back': 12,    \n",
    "    'null' : 0\n",
    "})\n",
    "\n",
    "# Variable categorica stat_fe a numérica\n",
    "df_transformado['stat_fe'] = df_transformado['stat_fe'].fillna('null').map({\n",
    "    'confirmed': 1,\n",
    "    'rejected': 2,    \n",
    "    'cancelled': 3 ,\n",
    "    'accepted': 4,\n",
    "    'null' : 0\n",
    "})\n",
    "\n",
    "# Descartar columnas repetidas\n",
    "df_transformado=df_transformado.drop(columns=['created_at_d','created_at_slot_h','Mes_created_at'])\n",
    "\n",
    "# Descartar columnas sin interés\n",
    "df_transformado=df_transformado.drop(columns=['reason'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformado.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Crear el escalador\n",
    "scaler = StandardScaler()\n",
    "needs=df_transformado['needs_m_check_recov']\n",
    "df_transformado=df_transformado.drop(columns='needs_m_check_recov')\n",
    "\n",
    "# Estandarizar todas las columnas\n",
    "normalizado = scaler.fit_transform(df_transformado)\n",
    "\n",
    "# Convertir de nuevo a DataFrame, preservando nombres de columnas e índices\n",
    "df_transformado = pd.DataFrame(normalizado, columns=df_transformado.columns, index=df_transformado.index)\n",
    "\n",
    "df_transformado.fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "df_transformado.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separación correcta de X e y\n",
    "X = df_transformado[['user_id', 'stat_cr','stat_fe','amount','fee','recovery_status','GBP_EUR','inflation','BTC_GBP','unemploy_rate','created_at_slot','created_at_dow','transfer_type','charge_moment_numeric']] # Elimina la columna objetivo\n",
    "y = needs  # La columna de la variable objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener los nombres de las características\n",
    "feature_names = X.columns\n",
    "\n",
    "# Divide los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=42)\n",
    "\n",
    "# Crea el clasificador de regresión logística. El 'liblinear' usa metodo de optimización de minimos cuadrados generalizados (L2) y soporta la regularización L1 y L2\n",
    "clf = LogisticRegression(solver=\"liblinear\")\n",
    "\n",
    "# Entrena el clasificador\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Realiza predicciones sobre el conjunto de prueba\n",
    "predicciones = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genera las probabilidades de predicción\n",
    "predicciones_probabilidades = clf.predict_proba(X_test)\n",
    "\n",
    "# Muestra las primeras 10 probabilidades de predicción\n",
    "#predicciones_probabilidades[:10]\n",
    "\n",
    "# Obtén los coeficientes y asigna los nombres de las características\n",
    "coeficientes = clf.coef_[0]  # clf.coef_ es un array bidimensional, tomamos la primera fila\n",
    "feature_coef = list(zip(feature_names, coeficientes))\n",
    "\n",
    "# Ordena las características por el valor absoluto del coeficiente en orden descendente\n",
    "feature_coef_sorted = sorted(feature_coef, key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "# Imprime cada variable junto con su coeficiente ordenado\n",
    "print(\"Coeficientes del modelo de regresión logística (ordenados por magnitud):\")\n",
    "for feature, coef in feature_coef_sorted:\n",
    "    print(f\"{feature}: {coef:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea un histograma de las probabilidades de predicción para ambas clases\n",
    "plt.figure(figsize=(10, 6)) \n",
    "\n",
    "# # Histograma para la clase negativa (need manual check)\n",
    "plt.hist(predicciones_probabilidades[:, 0], bins=20, color=\"skyblue\", edgecolor=\"black\", alpha=0.3, label=\"Necesita Manual check\")\n",
    "# Histograma para la clase positiva (no need manual check)\n",
    "plt.hist(predicciones_probabilidades[:, 1], bins=20, color=\"salmon\", edgecolor=\"black\", alpha=0.3, label=\"No necesita Manual check\")\n",
    "# Título y etiquetas de los ejes\n",
    "plt.title(\"Distribución de Probabilidades de Predicción para Ambas Clases\")\n",
    "plt.xlabel(\"Probabilidad de Predicción\")\n",
    "plt.ylabel(\"Frecuencia\")\n",
    "plt.legend(loc=\"upper center\") \n",
    "# Muestra la leyenda en el gráfico\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa la función para crear la matriz de confusión\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Evaluate model performance\n",
    "print(\"Accuracy:\", clf.score(X_test, y_test))  # Accuracy score\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, predicciones))  # Classification report\n",
    "\n",
    "\n",
    "# Genera la matriz de confusión\n",
    "matriz_confusion = confusion_matrix(y_test, predicciones)\n",
    "columnas = ['no_needs_m_check', 'needs_m_check'] # 0 para No necesita manual check y 1 para Sí necesita manual check\n",
    "\n",
    "# Visualiza la matriz de confusión utilizando un mapa de calor\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(matriz_confusion, annot=True, fmt='d', cmap='Blues', xticklabels=columnas, yticklabels=columnas)\n",
    "plt.xlabel(\"Predicción\")\n",
    "plt.ylabel(\"Realidad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "# MODELO DE ÁRBOL DE DECISIÓN\n",
    "\n",
    "# Dividir el dataset en entrenamiento y prueba\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crear el modelo de Árbol de Decisión\n",
    "DT = tree.DecisionTreeClassifier()\n",
    "\n",
    "# Entrenar el modelo con el conjunto de datos de entrenamiento\n",
    "DT.fit(train_X, train_y)\n",
    "\n",
    "# Realizar predicciones sobre el conjunto de prueba\n",
    "predictions = DT.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluación del rendimiento del modelo\n",
    "\n",
    "# 1. Precisión del modelo\n",
    "accuracy = DT.score(test_X, test_y)\n",
    "print(f\"Precisión del modelo: {accuracy:.4f}\")  # Precisión en el conjunto de prueba\n",
    "\n",
    "# 2. Reporte de clasificación\n",
    "print(\"\\nReporte de clasificación:\\n\", classification_report(test_y, predictions))  \n",
    "# Muestra el reporte con métricas como precisión, recall, f1-score para cada clase\n",
    "\n",
    "# 3. Importancia de las características\n",
    "\n",
    "# Extraer los nombres de las características\n",
    "feature_names = X.columns\n",
    "\n",
    "# Obtener las importancias de las características del modelo\n",
    "feature_importances = DT.feature_importances_\n",
    "\n",
    "# Crear una lista de tuplas con el nombre de la característica y su importancia\n",
    "feature_importance = list(zip(feature_names, feature_importances))\n",
    "\n",
    "# Ordenar las características por la importancia en orden descendente\n",
    "feature_importance_sorted = sorted(feature_importance, key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "# Imprimir las características ordenadas por importancia\n",
    "print(\"\\nImportancia de las características (ordenadas por magnitud):\")\n",
    "for feature, importance in feature_importance_sorted:\n",
    "    print(f\"{feature}: {importance:.4f}\")\n",
    "\n",
    "# 4. Visualización de la importancia de las características\n",
    "\n",
    "# Graficar las importancias de las características\n",
    "plt.figure(figsize=(10, 6))\n",
    "features = [f[0] for f in sorted(feature_importance, key=lambda x: abs(x[1]), reverse=False)]\n",
    "importances = [f[1] for f in sorted(feature_importance, key=lambda x: abs(x[1]), reverse=False)]\n",
    "\n",
    "# Crear el gráfico de barras\n",
    "plt.barh(features, importances, color='lightblue')\n",
    "plt.xlabel(\"Importancia\")\n",
    "plt.title(\"Importancia de las Características en el Modelo de Árbol de Decisión\")\n",
    "plt.show()\n",
    "\n",
    "# 5. Matriz de confusión\n",
    "\n",
    "# Nombres de las clases para la matriz de confusión\n",
    "columnas = ['no_needs_m_check', 'needs_m_check']  # 0 = No, 1 = Sí\n",
    "\n",
    "# Visualización de la matriz de confusión con un mapa de calor\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confusion_matrix(test_y, predictions), annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=columnas, yticklabels=columnas)\n",
    "\n",
    "# Personalización de los ejes\n",
    "plt.xlabel(\"Predicción\")\n",
    "plt.ylabel(\"Realidad\")\n",
    "plt.title(\"Matriz de Confusión\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Divide los datos\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crea el modelo XGBoost\n",
    "model = XGBClassifier(eval_metric='logloss')\n",
    "\n",
    "# Entrena el modelo\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Realiza predicciones\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calcula la precisión\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Precisión: {accuracy:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcula la matriz de confusión\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Visualiza la matriz de confusión con un mapa de calor\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['no_needs_m_check', 'needs_m_check'], yticklabels=['no_needs_m_check', 'needs_m_check'])\n",
    "plt.title('Matriz de Confusión')\n",
    "plt.xlabel('Predicción')\n",
    "plt.ylabel('Realidad')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
