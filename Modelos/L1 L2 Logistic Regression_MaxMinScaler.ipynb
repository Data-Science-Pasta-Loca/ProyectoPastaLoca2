{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de bibliotecas estándar\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Importación de bibliotecas para visualización\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "# Importando itertools para generar combinaciones de columnas\n",
    "import itertools \n",
    "\n",
    "# Importando la función seasonal_decompose para la descomposición de series temporales\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# Importación de herramientas para modelos de machine learning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "\n",
    "# Importación de módulo personalizado\n",
    "import payments_manager as pm\n",
    "\n",
    "cr_cp = pm.df('cr_cp')\n",
    "fe_cp = pm.df('fe_cp')\n",
    "df_jo = pm.df('df_jo')\n",
    "#df_jo = pm.sort(\"df_jo\", [\"id_cr\"]).reset_index()\n",
    "#df_jo = df_jo.drop(columns=['index'])\n",
    "#df_jo = df_jo.drop(columns=['Mes_created_at'])\n",
    "#df_jo_cp = df_jo.copy()\n",
    "df_jo = pm.sort(\"df_jo\", [\"id_cr\"]).reset_index()\n",
    "df_jo = df_jo.drop(columns=['index'])\n",
    "#df_jo.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datos de partida\n",
    "pd.options.display.max_columns = None\n",
    "pm.format_to_dates(df_jo, time_format='d') # 'min','s'\n",
    "df = pm.df('df_hyper')\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_log_L=df\n",
    "y_log_L=df_jo['needs_m_check_recov']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_log_L.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir las columnas de tipo timedelta64\n",
    "#for col in X_log_L.select_dtypes(include=['timedelta64']).columns:\n",
    "#    X_log_L[col] = X_log_L[col].apply(lambda x: x / pd.to_timedelta(1, unit='d') if pd.notnull(x) else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir las columnas de tipo datetime64\n",
    "#for col in X_log_L.select_dtypes(include=['datetime64']).columns:\n",
    "#    X_log_L[col] = X_log_L[col].apply(lambda x: x.timestamp() if pd.notnull(x) else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot econding para pasar a numéricas todas las categóricas\n",
    "X_log_L = pd.get_dummies(X_log_L, columns=['transfer_type'], drop_first=True, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_log_L = pd.get_dummies(X_log_L, columns= ['charge_moment'], drop_first=True, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rellenar NaN con 0\n",
    "X_log_L = X_log_L.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_log_L.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular la matriz de correlación\n",
    "m_corr = X_log_L.corr()\n",
    "\n",
    "# Visualizar la matriz de correlación\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(m_corr, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Matriz de Correlación Simple')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización datos antes del escalador\n",
    "display(X_log_L.head(10))\n",
    "\n",
    "# Sacamos la variable objetivo que no se va a escalar\n",
    "y = X_log_L['needs_m_check_recov'].copy()  #Segun moderated_at\n",
    "X_log_L = X_log_L.drop(columns=['needs_m_check_recov'])\n",
    "\n",
    "# Crear el escalador en este caso MINMAXSCALER\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Estandarizar todas las columnas\n",
    "normalizado = scaler.fit_transform(X_log_L)\n",
    "\n",
    "# Convertir de nuevo a DataFrame, preservando nombres de columnas e índices\n",
    "X = pd.DataFrame(normalizado, columns=X_log_L.columns, index=df.index)\n",
    "\n",
    "# Visualización de datos después del escalador\n",
    "display(X.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pasamos MODELO DE REGRESIÓN LOGISTICA BASE, con todas las caracteristicas para poder ver después la diferencia al aplicar Ridge y Lasso\n",
    "\n",
    "# Obtener los nombres de las características\n",
    "feature_names = X.columns\n",
    "\n",
    "# Divide los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=42)\n",
    "\n",
    "# Crea el clasificador de regresión logística. El 'liblinear' usa metodo de optimización de minimos cuadrados generalizados (L2) y soporta la regularización L1 y L2\n",
    "clf = LogisticRegression(solver=\"liblinear\")\n",
    "\n",
    "# Entrena el clasificador\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Realiza predicciones sobre el conjunto de prueba\n",
    "predicciones = clf.predict(X_test)\n",
    "\n",
    "# Genera las probabilidades de predicción\n",
    "predicciones_probabilidades = clf.predict_proba(X_test)\n",
    "\n",
    "# Muestra las primeras 10 probabilidades de predicción\n",
    "#predicciones_probabilidades[:10]\n",
    "\n",
    "# Obtén los coeficientes y asigna los nombres de las características\n",
    "coeficientes = clf.coef_[0]  # clf.coef_ es un array bidimensional, tomamos la primera fila\n",
    "feature_coef = list(zip(feature_names, coeficientes))\n",
    "\n",
    "# Ordena las características por el valor absoluto del coeficiente en orden descendente\n",
    "feature_coef_sorted = sorted(feature_coef, key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "# Imprime cada variable junto con su coeficiente ordenado\n",
    "print(\"Coeficientes del modelo de regresión logística (ordenados por magnitud):\")\n",
    "for feature, coef in feature_coef_sorted:\n",
    "    print(f\"{feature}: {coef:.8f}\")\n",
    "    \n",
    "# Convertir coeficientes a multiplicadores de odds\n",
    "feature_odds = [(feature, np.exp(coef)) for feature, coef in feature_coef_sorted]\n",
    "\n",
    "# Imprimir los resultados\n",
    "print(\"\\nMultiplicadores de odds (elevando exp a los coeficientes):\")\n",
    "for feature, odds in feature_odds:\n",
    "    print(f\"{feature}: {odds:.8f}\")\n",
    "    \n",
    "# Crea un histograma de las probabilidades de predicción para ambas clases\n",
    "plt.figure(figsize=(10, 6)) \n",
    "\n",
    "# # Histograma para la clase negativa (no need manual check)\n",
    "plt.hist(predicciones_probabilidades[:, 0], bins=20, color=\"skyblue\", edgecolor=\"black\", alpha=0.3, label=\"No necesita Manual check\")\n",
    "# Histograma para la clase positiva ( need manual check)\n",
    "plt.hist(predicciones_probabilidades[:, 1], bins=20, color=\"salmon\", edgecolor=\"black\", alpha=0.3, label=\"Necesita Manual check\")\n",
    "# Título y etiquetas de los ejes\n",
    "plt.title(\"Distribución de Probabilidades de Predicción para Ambas Clases\")\n",
    "plt.xlabel(\"Probabilidad de Predicción\")\n",
    "plt.ylabel(\"Frecuencia\")\n",
    "plt.legend(loc=\"upper center\") \n",
    "# Muestra la leyenda en el gráfico\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.1)\n",
    "plt.show()\n",
    "\n",
    "# Evaluate model performance\n",
    "print(\"Accuracy:\", clf.score(X_test, y_test))  # Accuracy score\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, predicciones))  # Classification report\n",
    "\n",
    "# Genera la matriz de confusión\n",
    "matriz_confusion = confusion_matrix(y_test, predicciones)\n",
    "columnas = ['no_needs_m_check', 'needs_m_check'] # 0 para No necesita manual check y 1 para Sí necesita manual check\n",
    "\n",
    "# Visualiza la matriz de confusión utilizando un mapa de calor\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(matriz_confusion, annot=True, fmt='d', cmap='Blues', xticklabels=columnas, yticklabels=columnas)\n",
    "plt.xlabel(\"Predicción\")\n",
    "plt.ylabel(\"Realidad\")\n",
    "plt.show()\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, predicciones_probabilidades[:, 1])\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {auc(fpr, tpr):.2f}\")\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probar modificando el umbral. Cambio a 0.4 (por ejemplo)\n",
    "umbral = 0.4\n",
    "predicciones_modificadas = (predicciones_probabilidades[:, 1] >= umbral).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model performance con umbral modificado\n",
    "print(\"Accuracy:\", clf.score(X_test, y_test))  # Accuracy score\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, predicciones_modificadas))  # Classification report\n",
    "\n",
    "# Genera la matriz de confusión\n",
    "matriz_confusion2 = confusion_matrix(y_test, predicciones_modificadas)\n",
    "columnas = ['no_needs_m_check', 'needs_m_check'] # 0 para No necesita manual check y 1 para Sí necesita manual check\n",
    "\n",
    "# Visualiza la matriz de confusión utilizando un mapa de calor\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(matriz_confusion2, annot=True, fmt='d', cmap='Blues', xticklabels=columnas, yticklabels=columnas)\n",
    "plt.xlabel(\"Predicción\")\n",
    "plt.ylabel(\"Realidad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar Penalizaciones Ridge y Lasso para Regresion Logistica al modelo base \n",
    "\n",
    "# Modelo con penalización L1 (similar a Lasso)\n",
    "logreg_l1 = LogisticRegression(penalty='l1', solver='saga', C=10, random_state=42, max_iter=1000) #probar tb con solver='liblinear'\n",
    "logreg_l1.fit(X_train, y_train)\n",
    "\n",
    "# Modelo con penalización L2 (similar a Ridge)\n",
    "logreg_l2 = LogisticRegression(penalty='l2', solver='lbfgs', C=10, random_state=42, max_iter=1000)\n",
    "logreg_l2.fit(X_train, y_train)\n",
    "\n",
    "# Predicciones\n",
    "y_pred_l1 = logreg_l1.predict(X_test)\n",
    "y_pred_l2 = logreg_l2.predict(X_test)\n",
    "\n",
    "# Evaluación (por ejemplo, exactitud)\n",
    "print(\"Exactitud L1 (LASSO):\", accuracy_score(y_test, y_pred_l1))\n",
    "print(\"Exactitud L2 (Ridge):\", accuracy_score(y_test, y_pred_l2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer los coeficientes para la L1\n",
    "coefs = logreg_l1.coef_[0]  # Para regresión logística binaria, es un vector 1D\n",
    "\n",
    "# Crear una lista con características y sus coeficientes\n",
    "feature_coef_l1 = list(zip(feature_names, coefs))\n",
    "\n",
    "# Ordenar las características por el valor absoluto del coeficiente en orden descendente\n",
    "feature_coef_l1_sorted = sorted(feature_coef_l1, key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "# Crear un DataFrame para visualizar los coeficientes junto con las características\n",
    "importancia = pd.DataFrame({\n",
    "    'Característica': feature_names,\n",
    "    'Coeficiente': coefs\n",
    "})\n",
    "\n",
    "# Ordenar por la magnitud de los coeficientes\n",
    "importancia = importancia.reindex(importancia['Coeficiente'].abs().sort_values(ascending=False).index)\n",
    "\n",
    "# Mostrar las características con coeficientes no nulos (para Lasso)\n",
    "importancia_no_nula = importancia[importancia['Coeficiente'] != 0]\n",
    "\n",
    "print(\"\\nCaracterísticas importantes para LASSO ordenadas por magnitud:\")\n",
    "print(importancia_no_nula)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraer los coeficientes para la L2\n",
    "coefs2 = logreg_l2.coef_[0]  # Para regresión logística binaria, es un vector 1D\n",
    "\n",
    "# Crear una lista con características y sus coeficientes\n",
    "feature_coef_l2 = list(zip(feature_names, coefs2))\n",
    "\n",
    "# Ordenar las características por el valor absoluto del coeficiente en orden descendente\n",
    "feature_coef_l2_sorted = sorted(feature_coef_l2, key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "# Crear un DataFrame para visualizar los coeficientes junto con las características\n",
    "importancia2 = pd.DataFrame({\n",
    "    'Característica': feature_names,\n",
    "    'Coeficiente': coefs2\n",
    "})\n",
    "\n",
    "# Ordenar por la magnitud de los coeficientes\n",
    "importancia2 = importancia2.reindex(importancia2['Coeficiente'].abs().sort_values(ascending=False).index)\n",
    "\n",
    "# Mostrar las características con coeficientes no nulos (para Lasso)\n",
    "importancia_no_nula2 = importancia2[importancia2['Coeficiente'] != 0]\n",
    "\n",
    "print(\"Características importantes para RIDGE:\")\n",
    "print(importancia_no_nula2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficamos los coeficientes ordenados por valor absoluto LASSO\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Convertir 'coefs' a un pandas.Series\n",
    "coefs_series_l1 = pd.Series(coefs, index=X_train.columns)\n",
    "\n",
    "# Ordenar los coeficientes de menor a mayor (en valor absoluto)\n",
    "coefs_series_l1 = coefs_series_l1.abs().sort_values(ascending=True)\n",
    "\n",
    "# graficar\n",
    "coefs_series_l1.plot(kind='bar', color=sns.color_palette(\"viridis\", n_colors=len(coefs_series_l1)))\n",
    "\n",
    "# Añadimos el título y etiquetas a los ejes\n",
    "plt.title('Coeficientes LASSO para Logistic Regression (Valor Absoluto)', fontsize=16, weight='bold')\n",
    "plt.xlabel('Características', fontsize=14)\n",
    "plt.ylabel('Valor Absoluto del Coeficiente', fontsize=14)\n",
    "# Ajustamos las etiquetas del eje x para mejorar la legibilidad\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "# Mostramos la gráfica con un diseño ajustado\n",
    "plt.tight_layout() # Ajuste del espaciado para evitar solapamientos\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficamos los coeficientes ordenados por valor absoluto RIDGE\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Convertir 'coefs' a un pandas.Series\n",
    "coefs_series_l2 = pd.Series(coefs2, index=X_train.columns)\n",
    "\n",
    "# Ordenar los coeficientes de menor a mayor (en valor absoluto)\n",
    "coefs_series_l2 = coefs_series_l2.abs().sort_values(ascending=True)\n",
    "\n",
    "# Ordenar los coeficientes de menor a mayor (invirtiendo el orden)\n",
    "coefs_series_l2.plot(kind='bar', color=sns.color_palette(\"viridis\", n_colors=len(coefs_series_l2)))\n",
    "\n",
    "# Añadimos el título y etiquetas a los ejes\n",
    "plt.title('Coeficientes Ridge para Logistic Regression (Valor Absoluto)', fontsize=16, weight='bold')\n",
    "plt.xlabel('Características', fontsize=14)\n",
    "plt.ylabel('Valor Absoluto del Odd', fontsize=14)\n",
    "# Ajustamos las etiquetas del eje x para mejorar la legibilidad\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "# Mostramos la gráfica con un diseño ajustado\n",
    "plt.tight_layout() # Ajuste del espaciado para evitar solapamientos\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encontrar C optima de manera manual.\n",
    "\n",
    "# Definir valores de C para probar\n",
    "C_values = np.logspace(-4, 4, 10)\n",
    "scores_l1 = []\n",
    "scores_l2 = []\n",
    "\n",
    "# Iterar manualmente para penalización L1\n",
    "for C in C_values:\n",
    "    model_l1 = LogisticRegression(\n",
    "        C=C, penalty='l1', solver='saga', max_iter=1000, random_state=42\n",
    "    )\n",
    "    score = cross_val_score(model_l1, X_train, y_train, cv=5, scoring='accuracy').mean()\n",
    "    scores_l1.append(score)\n",
    "\n",
    "# Iterar manualmente para penalización L2\n",
    "for C in C_values:\n",
    "    model_l2 = LogisticRegression(\n",
    "        C=C, penalty='l2', solver='saga', max_iter=1000, random_state=42\n",
    "    )\n",
    "    score = cross_val_score(model_l2, X_train, y_train, cv=5, scoring='accuracy').mean()\n",
    "    scores_l2.append(score)\n",
    "\n",
    "# Encontrar el mejor C\n",
    "optimal_C_l1 = C_values[np.argmax(scores_l1)]\n",
    "optimal_C_l2 = C_values[np.argmax(scores_l2)]\n",
    "\n",
    "print(f\"Mejor valor de C para penalización L1: {optimal_C_l1}\")\n",
    "print(f\"Mejor valor de C para penalización L2: {optimal_C_l2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buscamos el C optimo (el alpha optimo) para estas penalizaciones. Ahora con el LogisticRegressionCV que tiene Validación Cruzada incorporada.\n",
    "\n",
    "# Definir valores de C para probar\n",
    "C_values = np.logspace(-4, 4, 10)\n",
    "\n",
    "# Modelo con penalización L1\n",
    "logreg_l1 = LogisticRegressionCV(\n",
    "    Cs=C_values,  # Valores de C a probar\n",
    "    cv=5,         # Número de folds para validación cruzada\n",
    "    penalty='l1',\n",
    "    solver='saga', # Solver compatible con L1\n",
    "    max_iter=1000,\n",
    "    random_state=42\n",
    ")\n",
    "logreg_l1.fit(X_train, y_train)\n",
    "\n",
    "# Modelo con penalización L2\n",
    "logreg_l2 = LogisticRegressionCV(\n",
    "    Cs=C_values,  # Valores de C a probar\n",
    "    cv=5,         # Número de folds para validación cruzada\n",
    "    penalty='l2',\n",
    "    solver='saga', # Solver compatible con L2\n",
    "    max_iter=1000,\n",
    "    random_state=42\n",
    ")\n",
    "logreg_l2.fit(X_train, y_train)\n",
    "\n",
    "# Encontrar los valores óptimos de C\n",
    "optimal_C_l1 = logreg_l1.C_[0]\n",
    "optimal_C_l2 = logreg_l2.C_[0]\n",
    "\n",
    "print(f\"Mejor valor de C para penalización L1: {optimal_C_l1}\")\n",
    "print(f\"Mejor valor de C para penalización L2: {optimal_C_l2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar precisión promedio para cada C\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(C_values, logreg_l1.scores_[1].mean(axis=0), label='L1 (Lasso)', marker='o')\n",
    "plt.plot(C_values, logreg_l2.scores_[1].mean(axis=0), label='L2 (Ridge)', marker='o')\n",
    "\n",
    "plt.xscale('log')  # Escala logarítmica para C\n",
    "plt.xlabel('C (Regularización Inversa)')\n",
    "plt.ylabel('Precisión Promedio (Validación)')\n",
    "plt.title('Precisión vs C para L1 y L2')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que para C pequeñas (alphas grandes) la precisión promedio baja a 0.5 y 0.6, es decir que el modelo es sensible a la regularización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver los puntajes promedio por validación cruzada para cada C\n",
    "scores_l1 = logreg_l1.scores_[1].mean(axis=0)  # Precisión promedio para cada valor de C\n",
    "scores_l2 = logreg_l2.scores_[1].mean(axis=0)\n",
    "\n",
    "# Mostrar C óptimos\n",
    "print(\"Puntajes promedio para L1:\", scores_l1)\n",
    "print(\"Puntajes promedio para L2:\", scores_l2)\n",
    "\n",
    "# Comprobar el índice del mejor puntaje (C óptimo)\n",
    "optimal_index_l1 = scores_l1.argmax()\n",
    "optimal_index_l2 = scores_l2.argmax()\n",
    "\n",
    "print(f\"\\nÍndice óptimo para L1: {optimal_index_l1} \\nMejor C: {logreg_l1.Cs_[optimal_index_l1]}\")\n",
    "print(f\"\\nÍndice óptimo para L2: {optimal_index_l2} \\nMejor C: {logreg_l2.Cs_[optimal_index_l2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probamos MODELO REGRESION LOGISTICA con este analisis \n",
    "\n",
    "# Entrenar el modelo final con el valor óptimo de C\n",
    "logreg_l1_optimal = LogisticRegression(penalty='l1', solver='saga', C=optimal_C_l1, random_state=42, max_iter=1000)\n",
    "logreg_l2_optimal = LogisticRegression(penalty='l2', solver='saga', C=optimal_C_l2, random_state=42, max_iter=1000)\n",
    "\n",
    "# Entrenar los modelos con el valor de C óptimo\n",
    "logreg_l1_optimal.fit(X_train, y_train)\n",
    "logreg_l2_optimal.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar predicciones en el conjunto de prueba\n",
    "y_pred_l1 = logreg_l1_optimal.predict(X_test)\n",
    "y_pred_l2 = logreg_l2_optimal.predict(X_test)\n",
    "\n",
    "# Evaluar el desempeño del modelo L1\n",
    "print(\"Evaluación del modelo Lasso (L1):\")\n",
    "print(\"Precisión:\", accuracy_score(y_test, y_pred_l1))\n",
    "print(\"Reporte de clasificación L1:\\n\", classification_report(y_test, y_pred_l1))\n",
    "\n",
    "# Evaluar el desempeño del modelo L2\n",
    "print(\"Evaluación del modelo Ridge (L2):\")\n",
    "print(\"Precisión:\", accuracy_score(y_test, y_pred_l2))\n",
    "print(\"Reporte de clasificación L2:\\n\", classification_report(y_test, y_pred_l2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genera las probabilidades de predicción para ambos modelos\n",
    "predicciones_prob_l1 = logreg_l1_optimal.predict_proba(X_test)\n",
    "predicciones_prob_l2 = logreg_l2_optimal.predict_proba(X_test)\n",
    "\n",
    "# Obtén los coeficientes de ambos modelos\n",
    "coef_l1 = logreg_l1_optimal.coef_[0]  # Array unidimensional\n",
    "coef_l2 = logreg_l2_optimal.coef_[0]\n",
    "\n",
    "# Asocia los coeficientes con los nombres de las características\n",
    "feature_coef_l1 = list(zip(feature_names, coef_l1))\n",
    "feature_coef_l2 = list(zip(feature_names, coef_l2))\n",
    "\n",
    "# Ordena las características por valor absoluto del coeficiente\n",
    "feature_coef_l1_sorted = sorted(feature_coef_l1, key=lambda x: abs(x[1]), reverse=True)\n",
    "feature_coef_l2_sorted = sorted(feature_coef_l2, key=lambda x: abs(x[1]), reverse=True)\n",
    "\n",
    "# Imprime las características ordenadas por magnitud del coeficiente\n",
    "print(\"Coeficientes del modelo Ridge (L2) (ordenados por magnitud):\")\n",
    "for feature, coef in feature_coef_l2_sorted:\n",
    "    print(f\"{feature}: {coef:.8f}\")\n",
    "\n",
    "print(\"\\nCoeficientes del modelo Lasso (L1) (ordenados por magnitud):\")\n",
    "for feature, coef in feature_coef_l1_sorted:\n",
    "    print(f\"{feature}: {coef:.8f}\")\n",
    "\n",
    "# Crea histogramas para ambas clases y modelos\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.hist(predicciones_prob_l1[:, 0], bins=20, color=\"skyblue\", edgecolor=\"black\", alpha=0.3, label=\"Clase Negativa (Lasso)\")\n",
    "plt.hist(predicciones_prob_l1[:, 1], bins=20, color=\"salmon\", edgecolor=\"black\", alpha=0.3, label=\"Clase Positiva (Lasso)\")\n",
    "plt.title(\"Distribución de Probabilidades - Modelo Lasso (L1)\")\n",
    "plt.xlabel(\"Probabilidad de Predicción\")\n",
    "plt.ylabel(\"Frecuencia\")\n",
    "plt.legend(loc=\"upper center\")\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.1)\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.hist(predicciones_prob_l2[:, 0], bins=20, color=\"skyblue\", edgecolor=\"black\", alpha=0.3, label=\"Clase Negativa (Ridge)\")\n",
    "plt.hist(predicciones_prob_l2[:, 1], bins=20, color=\"salmon\", edgecolor=\"black\", alpha=0.3, label=\"Clase Positiva (Ridge)\")\n",
    "plt.title(\"Distribución de Probabilidades - Modelo Ridge (L2)\")\n",
    "plt.xlabel(\"Probabilidad de Predicción\")\n",
    "plt.ylabel(\"Frecuencia\")\n",
    "plt.legend(loc=\"upper center\")\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.1)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Evaluación de desempeño para ambos modelos\n",
    "print(\"Desempeño del Modelo Lasso (L1):\")\n",
    "print(\"Accuracy:\", logreg_l1_optimal.score(X_test, y_test))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_l1))\n",
    "\n",
    "print(\"\\nDesempeño del Modelo Ridge (L2):\")\n",
    "print(\"Accuracy:\", logreg_l2_optimal.score(X_test, y_test))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_l2))\n",
    "\n",
    "# Generar matrices de confusión para ambos modelos\n",
    "matriz_confusion_l1 = confusion_matrix(y_test, y_pred_l1)\n",
    "matriz_confusion_l2 = confusion_matrix(y_test, y_pred_l2)\n",
    "\n",
    "# Visualizar matrices de confusión usando mapas de calor\n",
    "plt.figure(figsize=(16, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.heatmap(matriz_confusion_l1, annot=True, fmt='d', cmap='Blues', xticklabels=columnas, yticklabels=columnas)\n",
    "plt.title(\"Matriz de Confusión - Modelo Lasso (L1)\")\n",
    "plt.xlabel(\"Predicción\")\n",
    "plt.ylabel(\"Realidad\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.heatmap(matriz_confusion_l2, annot=True, fmt='d', cmap='Blues', xticklabels=columnas, yticklabels=columnas)\n",
    "plt.title(\"Matriz de Confusión - Modelo Ridge (L2)\")\n",
    "plt.xlabel(\"Predicción\")\n",
    "plt.ylabel(\"Realidad\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparación resultados entre MODELO BASE, el BASE con cambio de umbral y el modelo entrenado según LASSO y RIDGE\n",
    "\n",
    "# ROC-AUC para cada modelo\n",
    "roc_auc_base = roc_auc_score(y_test, predicciones)\n",
    "roc_auc_umbral = roc_auc_score(y_test, predicciones_modificadas)\n",
    "roc_auc_ridge = roc_auc_score(y_test, y_pred_l2)\n",
    "roc_auc_lasso = roc_auc_score(y_test, y_pred_l1)\n",
    "\n",
    "# Base model\n",
    "accuracy_base = accuracy_score(y_test, predicciones)\n",
    "precision_base = precision_score(y_test, predicciones)\n",
    "recall_base = recall_score(y_test, predicciones)\n",
    "f1_base = f1_score(y_test, predicciones)\n",
    "roc_auc_base = roc_auc_score(y_test, predicciones)\n",
    "\n",
    "# Base model con umbral modificado\n",
    "accuracy_umbral = accuracy_score(y_test, predicciones_modificadas)\n",
    "precision_umbral = precision_score(y_test, predicciones_modificadas)\n",
    "recall_umbral = recall_score(y_test, predicciones_modificadas)\n",
    "f1_umbral = f1_score(y_test, predicciones_modificadas)\n",
    "roc_auc_umbral = roc_auc_score(y_test, predicciones_modificadas)\n",
    "\n",
    "# Ridge model\n",
    "accuracy_ridge = accuracy_score(y_test, y_pred_l2)\n",
    "precision_ridge = precision_score(y_test, y_pred_l2)\n",
    "recall_ridge = recall_score(y_test, y_pred_l2)\n",
    "f1_ridge = f1_score(y_test, y_pred_l2)\n",
    "roc_auc_ridge = roc_auc_score(y_test, y_pred_l2)\n",
    "\n",
    "# Lasso model\n",
    "accuracy_lasso = accuracy_score(y_test, y_pred_l1)\n",
    "precision_lasso = precision_score(y_test, y_pred_l1)\n",
    "recall_lasso = recall_score(y_test, y_pred_l1)\n",
    "f1_lasso = f1_score(y_test, y_pred_l1)\n",
    "roc_auc_lasso = roc_auc_score(y_test, y_pred_l1)\n",
    "\n",
    "# Comparar métricas clave en una tabla\n",
    "metricas = {\n",
    "    \"R_LOGISTICA MaxMinScaler\": [\"Base\", \"Umbral 0.4\",\"Ridge\", \"Lasso\"],\n",
    "    \"Accuracy\": [accuracy_base, accuracy_umbral ,accuracy_ridge, accuracy_lasso],\n",
    "    \"Precision\": [precision_base, precision_umbral,precision_ridge, precision_lasso],\n",
    "    \"Recall\": [recall_base, recall_umbral, recall_ridge, recall_lasso],\n",
    "    \"F1-Score\": [f1_base, f1_umbral, f1_ridge, f1_lasso],\n",
    "    \"ROC-AUC\": [roc_auc_base, roc_auc_umbral, roc_auc_ridge, roc_auc_lasso],\n",
    "}\n",
    "\n",
    "df_metricas = pd.DataFrame(metricas)\n",
    "print(df_metricas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar el C (que es 1/alpha)\n",
    "\n",
    "# Valores de C para probar (equivalente a 1/alpha en regresión logística)\n",
    "C_values = np.logspace(-4, 4, 10)\n",
    "\n",
    "# Inicialización de listas para almacenar resultados\n",
    "coefs_logreg_l1 = []\n",
    "coefs_logreg_l2 = []\n",
    "accuracy_l1 = []\n",
    "accuracy_l2 = []\n",
    "auc_l1 = []\n",
    "auc_l2 = []\n",
    "logloss_l1 = []\n",
    "logloss_l2 = []\n",
    "\n",
    "# Modelos y evaluación\n",
    "for C in C_values:\n",
    "    # Penalización L1\n",
    "    logreg_l1 = LogisticRegression(penalty='l1', C=C, solver='saga', max_iter=1000, random_state=42)\n",
    "    logreg_l1.fit(X_train, y_train)\n",
    "    coefs_logreg_l1.append(logreg_l1.coef_[0])  # Guardar coeficientes\n",
    "    y_pred_prob_l1 = logreg_l1.predict_proba(X_test)[:, 1]  # Probabilidades para ROC y log-loss\n",
    "    accuracy_l1.append(logreg_l1.score(X_test, y_pred_l1))\n",
    "    auc_l1.append(roc_auc_score(y_test, y_pred_prob_l1))  # AUC-ROC\n",
    "    logloss_l1.append(log_loss(y_test, y_pred_prob_l1))  # Log-loss\n",
    "\n",
    "    # Penalización L2\n",
    "    logreg_l2 = LogisticRegression(penalty='l2', C=C, solver='lbfgs', max_iter=1000, random_state=42)\n",
    "    logreg_l2.fit(X_train, y_train)\n",
    "    coefs_logreg_l2.append(logreg_l2.coef_[0])  # Guardar coeficientes\n",
    "    y_pred_prob_l2 = logreg_l2.predict_proba(X_test)[:, 1]\n",
    "    accuracy_l2.append(logreg_l2.score(X_test, y_pred_l2))\n",
    "    auc_l2.append(roc_auc_score(y_test, y_pred_prob_l2))  # AUC-ROC\n",
    "    logloss_l2.append(log_loss(y_test, y_pred_prob_l2))  # Log-loss\n",
    "\n",
    "\n",
    "# Configuración del gráfico\n",
    "fig, axs = plt.subplots(4, 2, figsize=(15, 15), sharey='row')\n",
    "\n",
    "# Definir los colores y estilos de línea\n",
    "colors = plt.cm.tab10.colors  # Paleta de colores\n",
    "line_styles = ['-', '--', '-.', ':']  # Diferentes tipos de línea\n",
    "\n",
    "# Coeficientes L1\n",
    "for i, coef in enumerate(np.abs(coefs_logreg_l1).T):\n",
    "    # El color se toma de la paleta y el estilo de línea se alterna según el índice\n",
    "    color = colors[i % 10]  # Asignar uno de los primeros 10 colores\n",
    "    line_style = line_styles[i // 10 % len(line_styles)]  # Alternar entre los diferentes tipos de línea cada 10\n",
    "    axs[0, 0].plot(C_values, coef, label=feature_names[i], color=color, linestyle=line_style)\n",
    "\n",
    "axs[0, 0].set_xscale('log')\n",
    "axs[0, 0].set_title('Coeficientes L1 en función de la regularización')\n",
    "axs[0, 0].set_xlabel('C')\n",
    "axs[0, 0].set_ylabel('Pesos')\n",
    "axs[0, 0].legend(feature_names)\n",
    "\n",
    "# Coloca la leyenda fuera del gráfico\n",
    "axs[0,0].legend(feature_names, loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "# Coeficientes L2\n",
    "for i, coef in enumerate(np.abs(coefs_logreg_l2).T):\n",
    "    # El color se toma de la paleta y el estilo de línea se alterna según el índice\n",
    "    color = colors[i % 10]  # Asignar uno de los primeros 10 colores\n",
    "    line_style = line_styles[i // 10 % len(line_styles)]  # Alternar entre los diferentes tipos de línea cada 10\n",
    "    axs[0, 1].plot(C_values, coef, label=feature_names[i], color=color, linestyle=line_style)\n",
    "\n",
    "axs[0, 1].set_xscale('log')\n",
    "axs[0, 1].set_title('Coeficientes L2 en función de la regularización')\n",
    "axs[0, 1].set_xlabel('C')\n",
    "axs[0, 1].set_ylabel('Pesos')\n",
    "axs[0, 1].legend(feature_names)\n",
    "\n",
    "# Coloca la leyenda fuera del gráfico\n",
    "axs[0,1].legend(feature_names, loc='upper left', bbox_to_anchor=(1, 1))\n",
    "\n",
    "#Graficar Accuracy para ver como varia un función de las C\n",
    "# Accuracy L1\n",
    "axs[1, 0].plot(C_values, accuracy_l1, marker='o')\n",
    "axs[1, 0].set_xscale('log')\n",
    "axs[1, 0].set_title('Accuracy L1 en función de la regularización')\n",
    "axs[1, 0].set_xlabel('C')\n",
    "axs[1, 0].set_ylabel('Accuracy')\n",
    "\n",
    "# Accuracy L2\n",
    "axs[1, 1].plot(C_values, accuracy_l2, marker='o')\n",
    "axs[1, 1].set_xscale('log')\n",
    "axs[1, 1].set_title('Accuracy L2 en función de la regularización')\n",
    "axs[1, 1].set_xlabel('C')\n",
    "axs[1, 1].set_ylabel('Accuracy')\n",
    "\n",
    "#Graficar AUC-ROC para ver como cambia rendimiento del modelo en términos de discriminación entre las clases postiva y negativa\n",
    "# AUC-ROC L1\n",
    "axs[2, 0].plot(C_values, auc_l1, marker='o')\n",
    "axs[2, 0].set_xscale('log')\n",
    "axs[2, 0].set_title('AUC-ROC L1 en función de la regularización')\n",
    "axs[2, 0].set_xlabel('C')\n",
    "axs[2, 0].set_ylabel('AUC-ROC')\n",
    "\n",
    "# AUC-ROC L2\n",
    "axs[2, 1].plot(C_values, auc_l2, marker='o')\n",
    "axs[2, 1].set_xscale('log')\n",
    "axs[2, 1].set_title('AUC-ROC L2 en función de la regularización')\n",
    "axs[2, 1].set_xlabel('C')\n",
    "axs[2, 1].set_ylabel('AUC-ROC')\n",
    "\n",
    "# Graficar Log-loss para ver como cambia la calidad de las predicciones (mide la perdida del modelo,los errores q hace)\n",
    "# Log-loss L1\n",
    "axs[3, 0].plot(C_values, logloss_l1, marker='o')\n",
    "axs[3, 0].set_xscale('log')\n",
    "axs[3, 0].set_title('Log-loss L1 en función de la regularización')\n",
    "axs[3, 0].set_xlabel('C')\n",
    "axs[3, 0].set_ylabel('Log-loss')\n",
    "\n",
    "# Log-loss L2\n",
    "axs[3, 1].plot(C_values, logloss_l2, marker='o')\n",
    "axs[3, 1].set_xscale('log')\n",
    "axs[3, 1].set_title('Log-loss L2 en función de la regularización')\n",
    "axs[3, 1].set_xlabel('C')\n",
    "axs[3, 1].set_ylabel('Log-loss')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar curvas de error. \n",
    "\n",
    "np.random.seed(42)\n",
    "# Parámetros\n",
    "MAXN = 5000 # len(X)  # Total de muestras disponibles\n",
    "steps = 10  # Tamaño de incremento del conjunto de entrenamiento\n",
    "iterations = 10  # Número de iteraciones para suavizar los resultados\n",
    "print(f\"Total de muestras: {MAXN}\")\n",
    "\n",
    "# Divide los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2) #, random_state=42)\n",
    "#display(X_test)\n",
    "#display(y_test)\n",
    "\n",
    "print(f\"{(MAXN // steps) * steps + 0 }\")\n",
    "\n",
    "# Calcular el número de columnas\n",
    "cols = list(range(steps, (MAXN // steps) * steps, steps))\n",
    "\n",
    "# Inicializamos los DataFrames para almacenar los errores\n",
    "test_errors = pd.DataFrame(np.zeros((iterations, len(cols))), columns=cols)\n",
    "train_errors = pd.DataFrame(np.zeros((iterations, len(cols))), columns=cols)\n",
    "\n",
    "# Generamos los errores promedios por iteración\n",
    "for iteration in range(iterations):\n",
    "    # Mezclamos los datos\n",
    "    data = pd.concat([X, y], axis=1).sample(frac=1).reset_index(drop=True)\n",
    "    X_shuffled = data[X.columns]\n",
    "    y_shuffled = data[y.name]\n",
    "    \n",
    "    for N in range(steps, MAXN, steps):\n",
    "        j = (N // steps) - 1  # Índice para almacenar resultados\n",
    "        \n",
    "        # Tomar un subconjunto de datos para entrenamiento\n",
    "        X_subset = X_shuffled.iloc[:N, :]\n",
    "        y_subset = y_shuffled.iloc[:N]\n",
    "        \n",
    "        # Crear y entrenar el modelo de regresión logística\n",
    "        clf = LogisticRegression(solver=\"liblinear\", max_iter=1000)\n",
    "        clf.fit(X_subset, y_subset)\n",
    "        \n",
    "        # Evaluar el modelo en el conjunto de prueba y calcular la tasa de error.\n",
    "        test_errors.iloc[iteration, j] = 1 - metrics.accuracy_score(clf.predict(X_test), y_test)\n",
    "        # Evaluar el modelo en el conjunto de entrenamiento y calcular la tasa de error.\n",
    "        train_errors.iloc[iteration, j] = 1 - metrics.accuracy_score(clf.predict(X_subset), y_subset)\n",
    "\n",
    "# Promediar las tasas de error a lo largo de las iteraciones para obtener una curva de aprendizaje más estable.\n",
    "mean_test_error = test_errors.mean(axis=0)\n",
    "mean_train_error = train_errors.mean(axis=0)\n",
    "\n",
    "# Graficar las curvas de aprendizaje: tasa de error en prueba y entrenamiento.\n",
    "plt.figure(figsize=(9, 5))\n",
    "plt.plot(mean_test_error, 'r', label='Error en Prueba') # Error en el conjunto de prueba.\n",
    "plt.plot(mean_train_error, 'b', label='Error en Entrenamiento') # Error en el conjunto de entrenamiento.\n",
    "\n",
    "# Configurar etiquetas, título y leyenda.\n",
    "plt.xlabel('Número de muestras x10', fontsize=12)\n",
    "plt.ylabel('Tasa de Error', fontsize=12)\n",
    "plt.title('Curvas de Aprendizaje para Regresion Logistica', fontsize=14)\n",
    "plt.legend(loc='upper left', fontsize=12, bbox_to_anchor=(1, 1))\n",
    "\n",
    "# Mostrar la gráfica con las curvas de error.\n",
    "plt.grid(True)\n",
    "plt.tight_layout() # Ajustar la distribución para evitar que las etiquetas se corten.\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
