{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegurar que las gráficas se muestren en línea en el Jupyter Notebook\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import itertools # Importando itertools para generar combinaciones de columnas\n",
    "# Importando la función seasonal_decompose para la descomposición de series temporales\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import matplotlib.patches as mpatches\n",
    "import payments_manager as pm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "# Importa la función para crear la matriz de confusión\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn import metrics # Para calcular métricas de rendimiento del modelo.\n",
    "from sklearn import tree\n",
    "\n",
    "#cr_cp = pm.df('cr_cp')\n",
    "#fe_cp = pm.df('fe_cp')\n",
    "df_jo = pm.df('df_jo')\n",
    "#df_jo = pm.sort(\"df_jo\", [\"id_cr\"]).reset_index()\n",
    "#df_jo = df_jo.drop(columns=['index'])\n",
    "#df_jo = df_jo.drop(columns=['Mes_created_at'])\n",
    "#df_jo_cp = df_jo.copy()\n",
    "df_jo = pm.sort(\"df_jo\", [\"id_cr\"]).reset_index()\n",
    "df_jo = df_jo.drop(columns=['index'])\n",
    "#df_jo.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amount</th>\n",
       "      <th>fee</th>\n",
       "      <th>n_backs</th>\n",
       "      <th>n_fees</th>\n",
       "      <th>n_incidents</th>\n",
       "      <th>to_reimbur</th>\n",
       "      <th>type_incident</th>\n",
       "      <th>type_instant_payment</th>\n",
       "      <th>type_postpone</th>\n",
       "      <th>transfer_type_regular</th>\n",
       "      <th>category_nice</th>\n",
       "      <th>category_rejected_direct_debit</th>\n",
       "      <th>charge_moment_before</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>16.376464</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   amount  fee  n_backs  n_fees  n_incidents  to_reimbur  type_incident  \\\n",
       "0     1.0  0.0        0       0            1   16.376464              0   \n",
       "\n",
       "   type_instant_payment  type_postpone  transfer_type_regular  category_nice  \\\n",
       "0                     0              0                      1              0   \n",
       "\n",
       "   category_rejected_direct_debit  charge_moment_before  \n",
       "0                               0                     0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = df_jo.copy()\n",
    "\n",
    "# Para utilizar modelos de árbol de decisión.\n",
    "# Establecer la semilla para la aleatorización (asegura resultados repetibles).\n",
    "np.random.seed(42)\n",
    "\n",
    "# Convertir columnas datetime a timestamps\n",
    "for col in df.select_dtypes(include=['datetime64']).columns:\n",
    "    df[col] = df[col].apply(lambda x: x.timestamp() if pd.notnull(x) else None)\n",
    "\n",
    "for col in df.select_dtypes(include=['timedelta64']).columns:\n",
    "    df[col] = df[col].apply(lambda x: x / pd.to_timedelta(1, unit='d') if pd.notnull(x) else None)\n",
    "\n",
    "# Variables predictoras (ajusta según los datos disponibles)\n",
    "columnas = ['amount','fee','n_backs','n_fees','n_incidents','to_reimbur', 'type', 'transfer_type','category','charge_moment'] #'user_id',   \n",
    "X = df[columnas].copy()\n",
    "\n",
    "# Categoricas\n",
    "X = pd.get_dummies(X, columns=['type', 'transfer_type', 'category','charge_moment'], drop_first=True, dtype =int)\n",
    "\n",
    "# Crear el escalador\n",
    "display(X.head(1))\n",
    "scaler = StandardScaler()\n",
    "# Estandarizar todas las columnas\n",
    "normalizado = scaler.fit_transform(X)\n",
    "# Convertir de nuevo a DataFrame, preservando nombres de columnas e índices\n",
    "X = pd.DataFrame(normalizado, columns=X.columns, index=df.index)\n",
    "X.fillna(0, inplace=True)\n",
    "#X.info()\n",
    "y = df['needs_m_check_recov'].copy()  #moderada # La columna de la variable objetivo\n",
    "\n",
    "# Divide los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=42)\n",
    "\n",
    "# Crea el clasificador de regresión logística. El 'liblinear' usa metodo de optimización de minimos cuadrados generalizados (L2) y soporta la regularización L1 y L2\n",
    "clf = LogisticRegression(solver=\"liblinear\")\n",
    "clf.fit(X_train, y_train) # Entrena el clasificador\n",
    "predicciones = clf.predict(X_test) # Realiza predicciones sobre el conjunto de prueba\n",
    "\n",
    "# Genera las probabilidades de predicción\n",
    "predicciones_probabilidades = clf.predict_proba(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidIndexError",
     "evalue": "(slice(None, 10, None), slice(None, None, None))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/tensorflow_env/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: (slice(None, 10, None), slice(None, None, None))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidIndexError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 44\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iteration \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# # Generar datos sintéticos para el conjunto de entrenamiento.\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# # Usar distribuciones normales para crear datos representativos.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m \n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# Evaluar el modelo con diferentes tamaños de muestra.\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j, N \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m3000\u001b[39m, \u001b[38;5;241m10\u001b[39m)):\n\u001b[1;32m     43\u001b[0m         \u001b[38;5;66;03m# Tomar un subconjunto de datos para entrenamiento con los primeros N ejemplos.\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m         X_subset \u001b[38;5;241m=\u001b[39m \u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     45\u001b[0m         y_subset \u001b[38;5;241m=\u001b[39m y_train[:N]\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;66;03m# Crear y entrenar un clasificador de árbol de decisión con una profundidad máxima de C (ahora 5).\u001b[39;00m\n",
      "File \u001b[0;32m~/tensorflow_env/lib/python3.12/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/tensorflow_env/lib/python3.12/site-packages/pandas/core/indexes/base.py:3811\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[0;32m-> 3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m   3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n",
      "\u001b[0;31mInvalidIndexError\u001b[0m: (slice(None, 10, None), slice(None, None, None))"
     ]
    }
   ],
   "source": [
    "# Establecer la complejidad del árbol de decisión en 5 (profundidad máxima de 5).\n",
    "C = 5 # Profundidad máxima del árbol de decisión (establecido en 5).\n",
    "MAXN = 1000 # Número máximo de muestras por clase.\n",
    "\n",
    "# Inicializar matrices para almacenar las tasas de error para 10 iteraciones y diferentes números de muestras de entrenamiento.\n",
    "yhat_test_c5 = np.zeros((10, 299, 2)) # Error en prueba (tamaño:[iteraciones, tamaños de muestra, conjunto])\n",
    "yhat_train_c5 = np.zeros((10, 299, 2)) # Error en entrenamiento\n",
    "\n",
    "# Ejecutar el experimento 10 veces para obtener curvas suavizadas promedio.\n",
    "for iteration in range(10):\n",
    "    # Generar datos sintéticos para el conjunto de entrenamiento.\n",
    "    # Usar distribuciones normales para crear datos representativos.\n",
    "    X_train = np.concatenate([\n",
    "        1.25 * np.random.randn(MAXN, 2), # Primera distribución\n",
    "        5 + 1.5 * np.random.randn(MAXN, 2), # Segunda distribución\n",
    "        [8, 5] + 1.5 * np.random.randn(MAXN, 2) # Tercera distribución\n",
    "    ])\n",
    "    y_train = np.concatenate([\n",
    "        np.ones((MAXN, 1)), # Etiquetas para la primera clase\n",
    "        -np.ones((MAXN, 1)), # Etiquetas para la segunda clase\n",
    "        np.ones((MAXN, 1)) # Etiquetas para la tercera clase\n",
    "    ])\n",
    "\n",
    "    # Aleatorizar los datos para asegurar la variabilidad.\n",
    "    perm = np.random.permutation(y_train.size)\n",
    "    X_train = X_train[perm, :]\n",
    "    y_train = y_train[perm]\n",
    "\n",
    "    # Generar datos sintéticos para el conjunto de prueba.\n",
    "    X_test = np.concatenate([\n",
    "        1.25 * np.random.randn(MAXN, 2),\n",
    "        5 + 1.5 * np.random.randn(MAXN, 2),\n",
    "        [8, 5] + 1.5 * np.random.randn(MAXN, 2)\n",
    "    ])\n",
    "    y_test = np.concatenate([\n",
    "        np.ones((MAXN, 1)),\n",
    "        -np.ones((MAXN, 1)),\n",
    "        np.ones((MAXN, 1))\n",
    "    ])\n",
    "\n",
    "    # Evaluar el modelo con diferentes tamaños de muestra.\n",
    "    for j, N in enumerate(range(10, 3000, 10)):\n",
    "        # Tomar un subconjunto de datos para entrenamiento con los primeros N ejemplos.\n",
    "        X_subset = X_train[:N, :]\n",
    "        y_subset = y_train[:N]\n",
    "        # Crear y entrenar un clasificador de árbol de decisión con una profundidad máxima de C (ahora 5).\n",
    "        clf = tree.DecisionTreeClassifier(min_samples_leaf=1, max_depth=C)\n",
    "        clf.fit(X_subset, y_subset.ravel()) # Ajustar el modelo a los datos de entrenamiento.\n",
    "        # Evaluar el modelo en el conjunto de prueba y calcular la tasa de error.\n",
    "        yhat_test_c5[iteration, j, 1] = 1 -  metrics.accuracy_score(clf.predict(X_test), y_test.ravel())\n",
    "        # Evaluar el modelo en el conjunto de entrenamiento y calcular la tasa de error.\n",
    "        yhat_train_c5[iteration, j, 1] = 1 -  metrics.accuracy_score(clf.predict(X_subset), y_subset.ravel())\n",
    "\n",
    "# Promediar las tasas de error a lo largo de las iteraciones para obtener una curva de aprendizaje más estable.\n",
    "mean_test_error_c5 = np.mean(yhat_test_c5[:, :, 1].T, axis=1)\n",
    "mean_train_error_c5 = np.mean(yhat_train_c5[:, :, 1].T, axis=1)\n",
    "\n",
    "# Graficar las curvas de aprendizaje: tasa de error en prueba y entrenamiento.\n",
    "plt.figure(figsize=(9, 5))\n",
    "plt.plot(mean_test_error_c5, 'r', label='Error en Prueba') # Error en el conjunto de prueba.\n",
    "plt.plot(mean_train_error_c5, 'b', label='Error en Entrenamiento') # Error en el conjunto de entrenamiento.\n",
    "\n",
    "# Configurar etiquetas, título y leyenda.\n",
    "plt.xlabel('Número de muestras x10', fontsize=12)\n",
    "plt.ylabel('Tasa de Error', fontsize=12)\n",
    "plt.title('Curvas de Aprendizaje para un Árbol de Decisión con Profundidad Máxima = 5', fontsize=14)\n",
    "plt.legend(loc='upper left', fontsize=12, bbox_to_anchor=(1, 1))\n",
    "\n",
    "# Mostrar la gráfica con las curvas de error.\n",
    "plt.grid(True)\n",
    "plt.tight_layout() # Ajustar la distribución para evitar que las etiquetas se corten.\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
